# ML-Course-Notes

Notes for ML course at UCSF Library

## Visuals

### Decision Tree

<img src="decision_tree.png" alt="hi" class="inline"/>

### Random Forest

<img src="supervised_learning_visuals/random_forest.png" alt="hi" class="inline"/>

### Logistic Regression

<img src="logistic_regression.png" alt="hi" class="inline"/>

### Support Vector Machine

<img src="support_vector_machine.png" alt="hi" class="inline"/>

### Neural Net

<img src="neural_network.png" alt="hi" class="inline"/>

### Naive Bayes

<img src="naive_bayes.png" alt="hi" class="inline"/>

## Exercises

Swap out the Random Forest for a different ML algorithm. At their defaults, how do they perform?

See if you can get better performance from the Random Forest by changing some of the parameters. What does n_estimators do?

Take a look at the parameters for a vectorizer. What does ngram_range do? Can you view the output of the ngram range?

Try programming a rules based (as opposed to machine learning) approach. Can you beat these algorithms?
